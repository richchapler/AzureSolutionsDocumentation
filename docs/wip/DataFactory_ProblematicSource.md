### Lab: Identifying Problematic Records in Azure Data Factory Pipeline

This lab demonstrates how to replicate the functionality to identify and display problematic input data in Azure Data Factory (ADF). We'll create a simple pipeline with a copy activity that fails due to a problematic record, and implement a mechanism to isolate the issue.

---

### **Objective**

- Create a sample dataset with one problematic record
- Build an ADF pipeline that processes the dataset
- Configure error handling to capture and identify the problematic record

---

### **Steps**

#### **Step 1: Create Sample Data**

1. Create a CSV file named `sample_data.csv` with the following content:
    ```csv
    ID,Name,Age
    1,John,30
    2,Jane,25
    3,,40
    4,Emily,not_a_number
    5,Michael,35
    ```
   - Record 3 has a missing value (`Name` is empty)
   - Record 4 has a non-numeric value in the `Age` field (`not_a_number`)

2. Upload the file to a storage account (e.g., Azure Blob Storage).

---

#### **Step 2: Set Up an Azure Data Factory Pipeline**

1. **Create a New Pipeline**  
   - In the Azure Data Factory UI, create a new pipeline named `DetectProblematicRecords`.

2. **Add a Copy Data Activity**  
   - Drag the **Copy Data** activity onto the pipeline canvas.
   - Configure the **Source** to point to the `sample_data.csv` file in Azure Blob Storage.
   - Configure the **Sink** to write to a temporary table in an Azure SQL Database or another data store.

3. **Enable Fault Tolerance**  
   - In the Copy Data activity settings:
     - Set **Fault Tolerance** to allow error tracking.
     - Enable the **Skip incompatible rows** option.
     - Specify an error log file path (e.g., a dedicated folder in Azure Blob Storage).

---

#### **Step 3: Configure Error Handling**

1. Add an **Error Handling Activity**:
   - Use a **Lookup** activity to read the error log file generated by the Copy Data activity.
   - The error log file will contain details of skipped or failed records.

2. Parse the error log to extract details:
   - Include fields like `row number`, `column name`, and `error description`.

---

#### **Step 4: Debug and Run**

1. **Run the Pipeline**:
   - Trigger the pipeline.
   - Observe the output in the **Sink** dataset and verify that problematic records are skipped.

2. **Verify the Error Log**:
   - Open the error log file in Azure Blob Storage to identify:
     - Row number of the problematic record
     - Error description (e.g., "Invalid data format" or "Missing value")

---

### **Expected Output**

- **Valid Records Processed**:  
  Records with IDs `1`, `2`, and `5` are successfully written to the Sink.
  
- **Error Log**:  
  The log contains entries similar to:
  ```
  Row 3: Missing value in 'Name'
  Row 4: Invalid value 'not_a_number' in 'Age'
  ```

---

### **Optional Enhancements**

- Use a **Data Flow** in ADF for more advanced row-level validation.
- Integrate Azure Monitor or Application Insights to track and alert on pipeline errors dynamically.

This setup allows you to identify problematic records efficiently during pipeline execution.
